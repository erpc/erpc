package upstream

import (
	"context"
	"math/rand"
	"sync"

	pb "github.com/envoyproxy/go-control-plane/envoy/service/ratelimit/v3"

	"github.com/envoyproxy/ratelimit/src/config"
	"github.com/envoyproxy/ratelimit/src/limiter"
	"github.com/envoyproxy/ratelimit/src/stats"
	"github.com/envoyproxy/ratelimit/src/utils"
)

// memoryRateLimitCache implements limiter.RateLimitCache using an in-process
// sharded, per-window bucketed map. Keys generated by BaseRateLimiter already
// include a time-sliced suffix. We avoid O(N) cleanups by grouping counters by
// their window expiry and rotating whole buckets when the window passes.
type memoryRateLimitCache struct {
	shards  []bucketShard
	base    *limiter.BaseRateLimiter
	timeSrc utils.TimeSource
}

type bucketShard struct {
	mu        sync.Mutex
	buckets   map[int64]map[string]uint64 // expiry -> (key -> counter)
	lastPrune int64                       // last unix second we pruned
}

const rlDefaultShards = 64

func (m *memoryRateLimitCache) shardFor(key string) *bucketShard {
	// FNV-1a 64-bit hash to spread keys across shards
	const (
		offset64 = 1469598103934665603
		prime64  = 1099511628211
	)
	var h uint64 = offset64
	for i := 0; i < len(key); i++ {
		h ^= uint64(key[i])
		h *= prime64
	}
	idx := int(h % uint64(len(m.shards)))
	return &m.shards[idx]
}

func NewMemoryRateLimitCache(
	timeSource utils.TimeSource,
	jitterRand *rand.Rand,
	expirationJitterMaxSeconds int64,
	nearLimitRatio float32,
	cacheKeyPrefix string,
	statsManager stats.Manager,
) limiter.RateLimitCache {
	m := &memoryRateLimitCache{
		shards:  make([]bucketShard, rlDefaultShards),
		base:    limiter.NewBaseRateLimit(timeSource, jitterRand, expirationJitterMaxSeconds, nil, nearLimitRatio, cacheKeyPrefix, statsManager),
		timeSrc: timeSource,
	}
	for i := range m.shards {
		m.shards[i].buckets = make(map[int64]map[string]uint64)
	}
	return m
}

func (m *memoryRateLimitCache) DoLimit(
	ctx context.Context,
	request *pb.RateLimitRequest,
	limits []*config.RateLimit,
) []*pb.RateLimitResponse_DescriptorStatus {
	hitsAddends := utils.GetHitsAddends(request)
	cacheKeys := m.base.GenerateCacheKeys(request, limits, hitsAddends)

	now := m.timeSrc.UnixNow()
	statuses := make([]*pb.RateLimitResponse_DescriptorStatus, len(request.Descriptors))

	for i, ck := range cacheKeys {
		if ck.Key == "" {
			// No limit for this descriptor
			statuses[i] = m.base.GetResponseDescriptorStatus("", limiter.NewRateLimitInfo(limits[i], 0, 0, 0, 0), false, hitsAddends[i])
			continue
		}

		// Determine window expiry
		div := utils.UnitToDivider(limits[i].Limit.Unit)
		expiry := (now/div)*div + div

		sh := m.shardFor(ck.Key)
		sh.mu.Lock()
		// Prune expired buckets at most once per second per shard
		if sh.lastPrune != now {
			for ex := range sh.buckets {
				if ex <= now {
					delete(sh.buckets, ex)
				}
			}
			sh.lastPrune = now
		}
		counters := sh.buckets[expiry]
		if counters == nil {
			counters = make(map[string]uint64)
			sh.buckets[expiry] = counters
		}
		before := counters[ck.Key]
		after := before + hitsAddends[i]
		counters[ck.Key] = after
		sh.mu.Unlock()

		li := limiter.NewRateLimitInfo(limits[i], before, after, 0, 0)
		statuses[i] = m.base.GetResponseDescriptorStatus(ck.Key, li, false, hitsAddends[i])
	}

	return statuses
}

func (m *memoryRateLimitCache) Flush() {}
