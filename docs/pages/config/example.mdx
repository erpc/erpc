---
description: Example configs for erRPC (yaml/typescript)
---

import { Callout, Tabs, Tab } from "nextra/components";

# Complete config example

This example config demonstrates all features of eRPC in one place. For more explanation of each section, refer to dedicated pages:

- [Database](/config/database/drivers): to configure caching and database.
- [Projects](/config/projects): to define multiple projects with different rate limit budgets.
- [Networks](/config/projects/networks): to configure failsafe policies for each network.
- [Upstreams](/config/projects/upstreams): to configure upstreams with failsafe policies, rate limiters, allowed/rejected methods, etc.
- [Rate limiters](/config/rate-limiters): to configure various self-imposed budgets to prevent pressure on upstreams.
- [Failsafe](/config/failsafe): explains different policies such as retry, timeout, and hedges, used for networks and upstreams.

By default `erpc` binary will look for `./erpc.ts`, `./erpc.yaml`, `./erpc.yml` files in the current directory. You can change this path by passing an argument to the binary:

<Tabs items={["yaml", "typescript"]} defaultIndex={0} storageKey="GlobalConfigTypeTabIndex">
  <Tabs.Tab>
  ```bash
  $ erpc /path/to/your/erpc.yaml
  ```
  </Tabs.Tab>
  <Tabs.Tab>
  ```bash
  $ erpc /path/to/your/erpc.ts
  ```
  </Tabs.Tab>
</Tabs>

### Minimal config example

eRPC will auto-detect or use sane defaults for various configs such as retries, timeouts, circuit-breaker, hedges, node architecture etc.

<Callout type="info">
  <b>eRPC is Multi-chain</b><br />
  A single instance of eRPC can server multiple projects (frontend, indexer, etc) and multiple chains.
</Callout>

<Tabs items={["yaml", "typescript"]} defaultIndex={0} storageKey="GlobalConfigTypeTabIndex">
  <Tabs.Tab>
```yaml filename="erpc.yaml"
logLevel: debug
projects:
 - id: main
   upstreams:
   # Put all your RPC endpoints for any network here.
   # You don't need to define architecture (e.g. evm) or chain id (e.g. 42161)
   # as they will be detected automatically by eRPC.
   - endpoint: https://xxxxx.matic.quiknode.pro/xxxxxxxxxx/
   - endpoint: drpc://XXX_MY_DRPC.ORG_API_KEY_XXX # Add all supported chains of drpc.org
   - endpoint: blastapi://XXX_MY_BLASTAPI.IO_API_KEY_XXX # Add all supported chains of blastapi.io
   - endpoint: alchemy://XXX_MY_ALCHEMY.COM_API_KEY_XXX # Add all supported chains of alchemy.com
   - endpoint: envio://rpc.hypersync.xyz # Add all supported methods and chains of envio.dev HyperRPC
```
</Tabs.Tab>
  <Tabs.Tab>
```ts filename="erpc.ts"
import { createConfig } from "@erpc-cloud/config";

export default createConfig({
  logLevel: "debug",
  projects: [
    {
      id: "main",
      upstreams: [
        // Put all your RPC endpoints for any network here.
        // You don't need to define architecture (e.g. evm) or chainId (e.g., 42161)
        // as they will be detected automatically by eRPC.
        {
          endpoint: "https://xxxxx.matic.quiknode.pro/xxxxxxxxxx/",
        },
        {
          endpoint: `drpc://${process.env.DRPC_API_KEY}`, // Add all supported chains of drpc.org
        },
        {
          endpoint: `blastapi://${process.env.BLASTAPI_API_KEY}`, // Add all supported chains of blastapi.io
        },
        {
          endpoint: `alchemy://${process.env.ALCHEMY_API_KEY}`, // Add all supported chains of alchemy.com
        },
        {
          endpoint: "envio://rpc.hypersync.xyz", // Add all supported methods and chains of envio.dev HyperRPC
        },
      ],
    },
  ],
});

```
</Tabs.Tab>
</Tabs>

### Full config example

To have more control over the configuration, you can use the example below.

<Tabs items={["yaml", "typescript"]} defaultIndex={0} storageKey="GlobalConfigTypeTabIndex">
  <Tabs.Tab>
```yaml filename="erpc.yaml"
# Log level helps in debugging or error detection:
# - debug: information down to actual request and responses, and decisions about rate-liming etc.
# - info: usually prints happy paths and might print 1 log per request indicating of success or failure.
# - warn: these problems do not cause end-user problems, but might indicate degredataion or an issue such as cache databse being down.
# - error: these are problems that have end-user impact, such as misconfigurations.
logLevel: warn

# The main server for eRPC to listen for requests.
server:
  listenV4: true
  httpHostV4: "0.0.0.0"
  listenV6: false
  httpHostV6: "[::]"
  httpPort: 4000
  maxTimeout: 30s
  readTimeout: 10s
  writeTimeout: 20s
  enableGzip: true
  waitBeforeShutdown: 30s
  waitAfterShutdown: 30s
  tls:
    enabled: false
    certFile: "/path/to/cert.pem"
    keyFile: "/path/to/key.pem"
    caFile: "/path/to/ca.pem"  # Optional, for client cert verification
    insecureSkipVerify: false  # Optional, defaults to false

# Optional Prometheus metrics server
metrics:
  enabled: true
  listenV4: true
  hostV4: "0.0.0.0"
  listenV6: false
  hostV6: "[::]"
  port: 4001

# There are various use-cases of database in erpc, such as caching, dynamic configs, rate limit persistence, etc.
database:
  # `evmJsonRpcCache` defines the destination for caching JSON-RPC cals towards any EVM architecture upstream.
  # This database is non-blocking on critical path, and is used as best-effort.
  # Make sure the storage requirements meet your usage, for example caching 70m blocks + 10m txs + 10m traces on Arbitrum needs 200GB of storage.
  evmJsonRpcCache:
    # Refer to "Database" section for more details.
    # Note that table, schema and indexes will be created automatically if they don't exist.
    connectors:
      - id: memory-cache
        driver: memory
        memory:
          maxItems: 100000
      - id: postgres-cache
        driver: postgresql
        postgresql:
          connectionUri: >-
            postgres://YOUR_USERNAME_HERE:YOUR_PASSWORD_HERE@your.postgres.hostname.here.com:5432/your_database_name
          table: rpc_cache
    policies:
      - network: "*"
        method: "*"
        finality: finalized
        connector: memory-cache
        ttl: 0
      - network: "*"
        method: "*"
        finality: unfinalized
        connector: memory-cache
        maxItemSize: 1MB # optional max size of item to store via this policy
        ttl: 5s
      - network: "*"
        method: "*"
        finality: unknown
        connector: memory-cache
        ttl: 5s
      - network: "*" # supports * as wildcard and | as OR operator
        method: "eth_getLogs|trace_*" # supports * as wildcard and | as OR operator
        finality: finalized
        connector: postgres-cache
        ttl: 0
      - network: "evm:42161|evm:10" # supports * as wildcard and | as OR operator
        method: "arbtrace_*" # supports * as wildcard and | as OR operator
        finality: finalized
        connector: postgres-cache
        ttl: 86400s

# Each project is a collection of networks and upstreams.
# For example "backend", "indexer", "frontend", and you want to use only 1 project you can name it "main"
# The main purpose of multiple projects is different failsafe policies (more aggressive and costly, or less costly and more error-prone)
projects:
  - id: main

    # Optionally you can define a self-imposed rate limite budget for each project
    # This is useful if you want to limit the number of requests per second or daily allowance.
    rateLimitBudget: frontend-budget

    # This array configures network-specific (a.k.a chain-specific) features.
    # For each network "architecture" and corresponding network id (e.g. evm.chainId) is required.
    # Remember defining networks is OPTIONAL, so only provide these only if you want to override defaults.
    networks:
      - architecture: evm
        evm:
          chainId: 1
        # Refer to "Failsafe" section for more details.
        # On network-level "timeout" is applied for the whole lifecycle of the request (including however many retries)
        failsafe:
        - matchMethod: "*"    # Default policy for all methods
          timeout:
            duration: 30s
          retry:
            maxAttempts: 3
            delay: 0ms
          # Defining a "hedge" is highly-recommended on network-level because if upstream A is being slow for
          # a specific request, it can start a new parallel hedged request to upstream B, for whichever responds faster.
          hedge:
            delay: 500ms
            maxCount: 1
          circuitBreaker:
            failureThresholdCount: 160 # 80% error rate
            failureThresholdCapacity: 200
            halfOpenAfter: 5m
            successThresholdCount: 3
            successThresholdCapacity: 3
      - architecture: evm
        evm:
          chainId: 42161
        failsafe:
        - matchMethod: "*"
          timeout:
            duration: 30s
          retry:
            maxAttempts: 3
            delay: 0ms
          hedge:
            delay: 500ms
            maxCount: 1

    # Each upstream supports 1 or more networks (chains)
    upstreams:
      - id: blastapi-chain-42161
        type: evm
        endpoint: https://arbitrum-one.blastapi.io/xxxxxxx-xxxxxx-xxxxxxx
        # Defines which budget to use when hadnling requests of this upstream.
        rateLimitBudget: global-blast
        # chainId is optional and will be detected from the endpoint (eth_chainId) but it is recommended to set it explicitly, for faster initialization.
        evm:
          chainId: 42161
        # Which methods must never be sent to this upstream:
        ignoreMethods:
          - "alchemy_*"
          - "eth_traceTransaction"
        # Refer to "Failsafe" section for more details:
        failsafe:
        - matchMethod: "*"    # Default policy for all methods
          timeout:
            duration: 15s
          retry:
            maxAttempts: 2
            delay: 500ms
            backoffMaxDelay: 3s
            backoffFactor: 1.2
            jitter: 0ms
      - id: blastapi-chain-1
        type: evm
        endpoint: https://eth-mainnet.blastapi.io/xxxxxxx-xxxxxx-xxxxxxx
        rateLimitBudget: global-blast
        evm:
          chainId: 1
        failsafe:
        - matchMethod: "*"
          timeout:
            duration: 15s
          retry:
            maxAttempts: 2
            delay: 500ms
            backoffMaxDelay: 3s
            backoffFactor: 1.2
            jitter: 0ms
      - id: quiknode-chain-42161
        type: evm
        endpoint: https://xxxxxx-xxxxxx.arbitrum-mainnet.quiknode.pro/xxxxxxxxxxxxxxxxxxxxxxxx/
        rateLimitBudget: global-quicknode
        # You can enable auto-ignoring unsupported methods, instead of defining them explicitly.
        # NOTE: some providers (e.g. dRPC) are not consistent with "unsupported method" responses,
        # so this feature might mark methods as unsupported that are actually supported!
        autoIgnoreUnsupportedMethods: true
        # To allow auto-batching requests towards the upstream, use these settings.
        # Remember if "supportsBatch" is false, you still can send batch requests to eRPC
        # but they will be sent to upstream as individual requests.
        jsonRpc:
          supportsBatch: true
          batchMaxSize: 10
          batchMaxWait: 100ms
        evm:
          chainId: 42161
        failsafe:
        - matchMethod: "*"
          timeout:
            duration: 15s
          retry:
            maxAttempts: 2
            delay: 500ms
            backoffMaxDelay: 3s
            backoffFactor: 1.2
            jitter: 0ms

        # "id" is a unique identifier to distinguish in logs and metrics.
      - id: alchemy-multi-chain-example
        # For certain known providers (such as Alchemy) you use a custom protocol name
        # which allows a single upstream to import "all chains" supported by that provider.
        # Note that these chains are hard-coded in the repo, so if they support a new chain eRPC must be updated.
        endpoint: alchemy://XXXX_YOUR_ALCHEMY_API_KEY_HERE_XXXX
        rateLimitBudget: global
        failsafe:
        - matchMethod: "*"
          timeout:
            duration: 15s
          retry:
            maxAttempts: 2
            delay: 500ms
            backoffMaxDelay: 3s
            backoffFactor: 1.2
            jitter: 0ms

# Rate limiter allows you to create "shared" budgets for upstreams.
# For example upstream A and B can use the same budget, which means both of them together must not exceed the defined limits.
rateLimiters:
  budgets:
    - id: default-budget
      rules:
        - method: "*"
          maxCount: 10000
          period: 1s
          waitTime: 100ms # Allow waiting up to 100ms for capacity to free up (Default is 0 meaning immediate error)
    - id: global-blast
      rules:
        - method: "*"
          maxCount: 1000
          period: 1s
    - id: global-quicknode
      rules:
        - method: "*"
          maxCount: 300
          period: 1s
    - id: frontend-budget
      rules:
        - method: "*"
          maxCount: 500
          period: 1s
```
</Tabs.Tab>
  <Tabs.Tab>
```ts filename="erpc.ts"
/**
 * Converted config in TypeScript. Copy and create erpc.ts so the binary automatically imports it.
 */
import { 
  createConfig,
  DataFinalityStateFinalized,
  DataFinalityStateUnfinalized
} from "@erpc-cloud/config";

export default createConfig({
  // Log level helps in debugging or error detection:
  // - debug: information down to actual request and responses, and decisions about rate-limiting etc.
  // - info: usually prints happy paths and might print 1 log per request indicating success or failure.
  // - warn: these problems do not cause end-user problems, but might indicate degradation or an issue such as cache database being down.
  // - error: these are problems that have end-user impact, such as misconfigurations.
  logLevel: "warn",

  // There are various use-cases of database in erpc, such as caching, dynamic configs, rate limit persistence, etc.
  database: {
    // `evmJsonRpcCache` defines the destination for caching JSON-RPC calls towards any EVM architecture upstream.
    // This database is non-blocking on the critical path and is used as best-effort.
    // Ensure that the storage requirements meet your usage, e.g., caching 70m blocks + 10m txs + 10m traces on Arbitrum needs 200GB of storage.
    evmJsonRpcCache: {
      // Refer to "Database" section for more details.
      // Note that table, schema, and indexes will be created automatically if they don't exist.
      connectors: [
        {
          id: "memory-cache",
          driver: "memory",
          memory: {
            maxItems: 100000,
          },
        },
        {
          id: "postgres-cache",
          driver: "postgresql",
          postgresql: {
            connectionUri: `postgres://YOUR_USERNAME_HERE:YOUR_PASSWORD_HERE@your.postgres.hostname.here.com:5432/your_database_name`,
            table: "rpc_cache",
          },
        },
      ],
      policies: [
        {
          network: "*",
          method: "*",
          finality: DataFinalityStateFinalized,
          connector: "memory-cache",
          ttl: 0,
        },
        {
          network: "*",
          method: "*",
          finality: DataFinalityStateUnfinalized,
          connector: "memory-cache",
          ttl: "5s",
        },
        {
          network: "*",
          method: "*",
          finality: DataFinalityStateUnknown,
          connector: "memory-cache",
          ttl: "5s",
        },
        {
          network: "*", // supports * as wildcard and | as OR operator
          method: "eth_getLogs|trace_*", // supports * as wildcard and | as OR operator
          finality: DataFinalityStateFinalized,
          connector: "postgres-cache",
          ttl: 0,
        },
        {
          network: "evm:42161|evm:10", // supports * as wildcard and | as OR operator
          method: "arbtrace_*", // supports * as wildcard and | as OR operator
          finality: DataFinalityStateFinalized,
          connector: "postgres-cache",
          ttl: "1d",
        },
      ],
    },
  },

  // The main server for eRPC to listen for requests.
  server: {
    listenV4: true,
    httpHostV4: "0.0.0.0",
    listenV6: false,
    httpHostV6: "[::]",
    httpPort: 4000,
    maxTimeout: "30s",
    enableGzip: true,
    waitBeforeShutdown: "30s",
    waitAfterShutdown: "30s",
    tls: {
      enabled: false,
      certFile: "/path/to/cert.pem",
      keyFile: "/path/to/key.pem",
      caFile: "/path/to/ca.pem", // Optional, for client cert verification
      insecureSkipVerify: false, // Optional, defaults to false
    },
  },

  // Optional Prometheus metrics server
  metrics: {
    enabled: true,
    listenV4: true,
    hostV4: "0.0.0.0",
    listenV6: false,
    hostV6: "[::]",
    port: 4001,
  },

  // Each project is a collection of networks and upstreams.
  // For example "backend", "indexer", "frontend", and you want to use only 1 project you can name it "main"
  // The main purpose of multiple projects is different failsafe policies (more aggressive and costly, or less costly and more error-prone)
  projects: [
    {
      id: "main",

      // Optionally you can define a self-imposed rate limit budget for each project
      // This is useful if you want to limit the number of requests per second or daily allowance.
      rateLimitBudget: "frontend-budget",

      // This array configures network-specific (a.k.a chain-specific) features.
      // For each network "architecture" and corresponding network id (e.g. evm.chainId) is required.
      // Remember defining networks is OPTIONAL, so only provide these only if you want to override defaults.
      networks: [
        {
          architecture: "evm",
          evm: {
            chainId: 1,
          },
          // Refer to "Failsafe" section for more details.
          // On network-level "timeout" is applied for the whole lifecycle of the request (including however many retries on upstreams)
          failsafe: [
            {
              matchMethod: "*",    // Default policy for all methods
              timeout: {
                duration: "30s",
              },
              retry: {
                maxAttempts: 3,
                delay: "0ms",
              },
              // Defining a "hedge" is highly-recommended on network-level because if upstream A is being slow for
              // a specific request, it can start a new parallel hedged request to upstream B, for whichever responds faster.
              hedge: {
                delay: "500ms",
                maxCount: 1,
              },
              circuitBreaker: {
                failureThresholdCount: 160, // 80% error rate
                failureThresholdCapacity: 200,
                halfOpenAfter: "5m",
                successThresholdCount: 3,
                successThresholdCapacity: 3,
              },
            }
          ],
        },
        {
          architecture: "evm",
          evm: {
            chainId: 42161,
          },
          failsafe: [
            {
              matchMethod: "*",
              timeout: {
                duration: "30s",
              },
              retry: {
                maxAttempts: 3,
                delay: "0ms",
              },
              hedge: {
                delay: "500ms",
                maxCount: 1,
              },
            }
          ],
        },
      ],

      // Each upstream supports 1 or more networks (chains)
      upstreams: [
        {
          id: "blastapi-chain-42161",
          type: "evm",
          endpoint: "https://arbitrum-one.blastapi.io/xxxxxxx-xxxxxx-xxxxxxx",
          // Defines which budget to use when handling requests of this upstream.
          rateLimitBudget: "global-blast",
          // chainId is optional and will be detected from the endpoint (eth_chainId) but it is recommended to set it explicitly, for faster initialization.
          evm: {
            chainId: 42161,
          },
          // Which methods must never be sent to this upstream:
          ignoreMethods: ["alchemy_*", "eth_traceTransaction"],
          // Refer to "Failsafe" section for more details:
          failsafe: [
            {
              matchMethod: "*",    // Default policy for all methods
              timeout: {
                duration: "15s",
              },
              retry: {
                maxAttempts: 2,
                delay: "500ms",
                backoffMaxDelay: "3s",
                backoffFactor: 1.2,
                jitter: "0ms",
              },
            }
          ],
        },
        {
          id: "blastapi-chain-1",
          type: "evm",
          endpoint: "https://eth-mainnet.blastapi.io/xxxxxxx-xxxxxx-xxxxxxx",
          rateLimitBudget: "global-blast",
          evm: {
            chainId: 1,
          },
          failsafe: [
            {
              matchMethod: "*",
              timeout: {
                duration: "15s",
              },
              retry: {
                maxAttempts: 2,
                delay: "500ms",
                backoffMaxDelay: "3s",
                backoffFactor: 1.2,
                jitter: "0ms",
              },
            }
          ],
        },
        {
          id: "quiknode-chain-42161",
          type: "evm",
          endpoint:
            "https://xxxxxx-xxxxxx.arbitrum-mainnet.quiknode.pro/xxxxxxxxxxxxxxxxxxxxxxxx/",
          rateLimitBudget: "global-quicknode",
          // You can enable auto-ignoring unsupported methods, instead of defining them explicitly.
          // NOTE: some providers (e.g. dRPC) are not consistent with "unsupported method" responses,
          // so this feature might mark methods as unsupported that are actually supported!
          autoIgnoreUnsupportedMethods: true,
          // To allow auto-batching requests towards the upstream, use these settings.
          // Remember if "supportsBatch" is false, you still can send batch requests to eRPC
          // but they will be sent to upstream as individual requests.
          jsonRpc: {
            supportsBatch: true,
            batchMaxSize: 10,
            batchMaxWait: "100ms",
          },
          evm: {
            chainId: 42161,
          },
          failsafe: [
            {
              matchMethod: "*",
              timeout: {
                duration: "15s",
              },
              retry: {
                maxAttempts: 2,
                delay: "500ms",
                backoffMaxDelay: "3s",
                backoffFactor: 1.2,
                jitter: "0ms",
              },
            }
          ],
        },
        // "id" is a unique identifier to distinguish in logs and metrics.
        {
          id: "alchemy-multi-chain-example",
          // For certain known providers (such as Alchemy) you use a custom protocol name
          // which allows a single upstream to import "all chains" supported by that provider.
          // Note that these chains are hard-coded in the repo, so if they support a new chain eRPC must be updated.
          endpoint: `alchemy://${process.env.ALCHEMY_API_KEY}`,
          rateLimitBudget: "global",
          failsafe: [
            {
              matchMethod: "*",
              timeout: {
                duration: "15s",
              },
              retry: {
                maxAttempts: 2,
                delay: "500ms",
                backoffMaxDelay: "3s",
                backoffFactor: 1.2,
                jitter: "0ms",
              },
            }
          ],
        },
      ],
    },
  ],

  // Rate limiter allows you to create "shared" budgets for upstreams.
  // For example upstream A and B can use the same budget, which means both of them together must not exceed the defined limits.
  rateLimiters: {
    budgets: [
      {
        id: "default-budget",
        rules: [
          {
            method: "*",
            maxCount: 10000,
            period: "1s",
          },
        ],
      },
      {
        id: "global-blast",
        rules: [
          {
            method: "*",
            maxCount: 1000,
            period: "1s",
          },
        ],
      },
      {
        id: "global-quicknode",
        rules: [
          {
            method: "*",
            maxCount: 300,
            period: "1s",
          },
        ],
      },
      {
        id: "frontend-budget",
        rules: [
          {
            method: "*",
            maxCount: 500,
            period: "1s",
          },
        ],
      },
    ],
  },
});

```
</Tabs.Tab>
</Tabs>
